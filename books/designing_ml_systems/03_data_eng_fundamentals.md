# Chapter 3: Data Engineering Fundamentals

Large data systems, even without ML, are complex. Data models define how data relates to eachother. Database define how data is stored and read from machines.

## Data Sources

ML systems can work with many different data sources, this section goes over different types.

### User input data

- data input explicitly by users (files, spreadsheets, etc.)
- If it's possible to input wrong data, users will do it.
- this data requires very heavy checking and validation

### System generated data

- logs, system outputs (like model predictions), results of different jobs,
- This data is much less likely to be malformed or wrong. Processing is less intensive than needs to be done on user data

This data can grow very fast and get out of control. Use time windows or low-access storage types to cut down on the cost of storing all this data

### internal databases

- generated by various services and enterprise applications (inventory, customer relationships, users, etc.)
- Much of this can be used directly by an ML model (ex: pulling in inventory data to know what is allowed to be predicted on)

### Third-party data

- `First party data` is data collected by your company on customers or users
- `second party data` is data collected by another company on their users that is made available to you
- `Third party data` is data companies collect on the public who aren't their direct customers

This data is the largest data source but can be difficult to use without cleaning. Vendors clean and sell third-party data for use.

## Data formats

Storing/persisting data is useful but can be expensive. It's important to think about the data and current and future uses when deciding how to store it.

Questions to consider:

- How do I store multimodal data (image and text)?
- Where do I store my data so that it's cheap and still fast to access?
- How do I store complex models so that they can be loaded and run correctly on different hardware?

### Data Serialization

This is the process of converting a data structure or object state into a format that can be stored or transmitted and reconstructed later. Need to consider human-readability, storage size, interface requirements, etc.

Many different methods each having their own benefits and drawbacks:

| Format       | Binary/Text           | Human-readable | Example use cases |
| ------------- |:-------------:| -----:|  -----:|
| JSON    | text| Yes | Everywhere |
| CSV      | text   |   Yes | Everywhere |
| Parquet | binary   |    No | Hadoop, AWS Redshift |
| Avro | binary   |    No  | Hadoop|
| Protobuf | binary   |     No  | Google, TensorFlow |
| Pickle | binary   |    No  | Python, Pytorch |

#### JSON

- Used everywhere, most programming languages can work with it
- human readable.
- Schemas are difficult to change once implemented
- Nonbinary so take up a lot of space

#### Row-major vs. Column-major formats

- CSV is row-major (elements in a row are stored next to eachother in memory), parquet is column-major (elements in a column are stored next to eachother in memory)
- This effects speed in accessing subsequent columns/rows. Subsequent data is faster for computers to access.
- Row-major is better for accessing examples, and column-major is better for accessing features
  - row major formats can be very slow when dealing with a large number of features
  - row major is better for doing a lot of writes, column major is better for doing a lot of column-based reads (like ML)
